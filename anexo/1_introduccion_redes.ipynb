{"cells":[{"cell_type":"markdown","metadata":{"id":"uLPOr-8b1WvD"},"source":["<a href=\"https://www.inove.com.ar\"><img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/PA%20Banner.png\" width=\"1000\" align=\"center\"></a>\n","\n","\n","# Redes neuronales artificiales (ANN)\n","\n","Programa creado para mostrar ejemplos prácticos de los visto durante la clase<br>\n","v1.1"]},{"cell_type":"markdown","metadata":{"id":"kMKvN3ZhcF0r"},"source":["### **Objetivos:**\n","\n","\n","*   Comprender la estructura de una red neuronal.\n","*   Configurar el modelo para el entrenamiento a través del método compile.\n","*   Diferenciar las redes neuronales categóricas de las multicategóricas.\n"]},{"cell_type":"markdown","metadata":{"id":"SBftKJxBVQ-9"},"source":["# Redes neuronales de una sola capa oculta (ANN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzvdYxQKO5uL"},"outputs":[],"source":["#Librerias a implementar\n","import os\n","import platform\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","\n","from  sklearn import  datasets"]},{"cell_type":"markdown","metadata":{"id":"EkFATpYfVUzT"},"source":["## 1 - Clasificación binaria"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6sITMb8AhHD"},"outputs":[],"source":["# Se construye un DataFrame llamado df_drugs, que contiene:\n","# un diccionario con tres claves y sus listas de valores\n","# Las claves representan los nombres de las columnas\n","df_drugs = pd.DataFrame({\n","      \"Age\": [0, 1, 2, 0],\n","      \"cholesterol\": [0, 1 , 1, 0],\n","      \"drug\": [0, 1, 1, 0]}\n","      )\n","df_drugs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRZ8sasGqw-d"},"outputs":[],"source":["# Se hace una copia del DataFrame df_drugs \n","# para trabajar utilizar la copia df_norm para normalizar los datos\n","df_norm = df_drugs.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9w5D7KfJlNM"},"outputs":[],"source":["# Se importa MinMaxScaler de la librería sklearn.preprocessing \n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Se crea el objeto scaler a partir de la clase MinMaxScaler()\n","scaler = MinMaxScaler()\n","\n","# Del DataFrama df_norm se implementa el operador .loc para editar las columnas 'Age', 'cholesterol' \n","# Para acceder a las columnas se utiliza los corchetes\n","# Los dos puntos (:) indican que se editarán todas las filas de la columna indicada\n","# Para normalizar se utilizá el objeto creado scaler\n","# y el método .fit_transform, que se encargará de normalizar\n","# Entre paréntesis se indica el nombre de la columna del DataFrame. Ej:df_norm[['Age']]\n","df_norm.loc[:, 'Age'] = scaler.fit_transform(df_norm[['Age']])\n","df_norm.loc[:, 'cholesterol'] = scaler.fit_transform(df_norm[['cholesterol']])\n","\n","# Ver las 5 primeras filas del DataFrame normalizado\n","df_norm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5RmIy_nBN5s"},"outputs":[],"source":["# Se separan los valores para X e y con el método .values\n","# Para los valores de X se mantienen la mayoria de las columnas a excepción de la columna 'drug'\n","# axis=1, parámetro para indicar que se elimine fila a fila\n","# y representará la columna objetivo, que es la columna que contiene las categorías conocidas para cada fila, que el la columna 'drug'\n","X = df_norm.drop('drug', axis=1).values\n","y = df_norm['drug'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIiOdDxjBgMl"},"outputs":[],"source":["# Se importa Dense de la librería tensorflow.keras.layers\n","from tensorflow.keras.layers import Dense\n","\n","# Se crea el objeto model a partir de la clase Sequential()\n","model = Sequential()\n","\n","# A partir del objeto creado se implementa el método add()\n","# Dentro de los () se indica que se ustilizará una capa Densa (Dense)\n","# Dentro de la capa densa se definen los parámetros:\n","# units=1, cantidad de neuronas\n","# activation='sigmoid', función de activación que define el corportamiento del modelo\n","# input_shape=(2,) cantidad de entradas, esto lo define la cantidad de columnas.\n","model.add(Dense(units=1, activation='sigmoid', input_shape=(2,)))\n","\n","# De model creado se puede acceder al sumario que muestra la estructura del modelo\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gre6J13JB9cF"},"outputs":[],"source":["# Configuración del modelo para el entrenamiento, implementando el método compile a partir del modelo creado.\n","# Se necesita indicar los parámetros:\n","# optimizer, nombre del optimizador (es el algoritmo que se encarga del descenso de gradiente estocástico)\n","# Fuente: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n","# loss, se llama función de pérdida, representa las categorías conocidas de las predicción. Al ser 'binary_crossentropy' la predicción tiene \n","# una salida con dos opciones.\n","# metrics, se define la métrica que evaluará el modelo durante el entrenamiento y las pruebas.\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.5),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Se entrena el modelo con el método fit\n","# Necesita definir los valores para X, y sumado a la cantidad de épocas que seria la iteraciones de entrenamiento.\n","history = model.fit(X, y, epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PlF_Xa6H91G"},"outputs":[],"source":["# Podemos observar los pesos asociados al modelo\n","model.get_weights()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1cdah5DMfZJ"},"outputs":[],"source":["# Evaluar el accuracy del modelo\n","accuracy = model.evaluate(X, y)[1]"]},{"cell_type":"markdown","metadata":{"id":"sOfKniHIfJhY"},"source":["## 2 - Red neuronal (ANN) & clasificación multicategórica"]},{"cell_type":"markdown","metadata":{"id":"DlSgbwfhe8CH"},"source":["### `Penguins dataset`:\n","El dataset **`Penguins`** es un dataset alternativo al clásico dataset de **`iris`**, el cual se lo utiliza para clasificación multicagórica (3 especies de pinguinos). Cada especie se caracteriza por su tamaño, como podrá ver en el dataset.<br> [Dataset source](https://www.kaggle.com/parulpandey/penguin-dataset-the-new-iris/data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUSRnV0Lfw5J"},"outputs":[],"source":["if os.access('penguins_dataset.csv', os.F_OK) is False:\n","    if platform.system() == 'Windows':\n","        !curl https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/penguins_dataset.csv > penguins_dataset.csv\n","    else:\n","        !wget penguins_dataset.csv https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/penguins_dataset.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmcKzLsPhrzr"},"outputs":[],"source":["# Se lee el archivo \"\"penguins_dataset.csv\"\" con Pandas\n","# utilizando el método .read_csv de pd\n","df2 = pd.read_csv(\"penguins_dataset.csv\")\n","\n","# Permite observar las cinco primeras filas del dataset df2\n","df2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjnQWSjviniZ"},"outputs":[],"source":["# Realizar una inspeccion del dataset en búsqueda de elementos faltantes\n","# Una vez descargado el archivo en Colab.\n","# Leerlo con Pandas y el método read_csv\n","# Una vez extraida toda la información se almacena en df\n","# A partir de df2 y el método describe(), mostrará la descripción estadistica básica del archivo que se guardará en des\n","# Crear una fila nueva llamada Nan en el DataFrame  des,\n","# que indica la cantidad de datos tipo Nan que tiene cada columna.\n","# Para crear una nueva fila, se utilizará el operador .loc, donde se indica el nombre\n","# de la nueva fila y con que valores se completará.\n","# La información será de los datos faltantes df2.isna().sum()\n","# Crear una fila nueva llamada %Nan en el DataFrame des,\n","# Esta fila se completará con los porcentajes de Nan encontrados en cada columna.\n","des = df2.describe()\n","des.loc['Nan'] = df2.isna().sum()\n","des.loc['%Nan'] = (df2.isna().mean())*100\n","des"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gi0mSFcd0BRa"},"outputs":[],"source":["# El archivo contiene filas344  y columnas 17\n","df2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oL_HZDNkieSL"},"outputs":[],"source":["# Filtrar el DataFrame df2 con 5 columnas: \"Species\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"\n","# Para acceder a los valores de varias columnas del DataFrame se hace a través de una lista con los nombres de las columnas df2[[\"Species\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n","df2_clean = df2[[\"Species\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]]\n","\n","# Una vez filtrado el  DataFrame df2,\n","# se elimina los datos faltantes con dropna()\n","df2_clean = df2_clean.dropna()\n","df2_clean.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9PMQAts1IGk"},"outputs":[],"source":["# Como los indices quedaron desordenados se reindexa los índices.\n","# Con el método .reset_index()\n","# Se indica como parámetros:\n","# drop=True, esto restablece el índice al índice entero predeterminado.\n","# inplace=True, para modificar el DataFrame en lugar de crear uno nuevo.\n","# Fuente: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html\n","\n","df2_clean.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTELI7MQjIfL"},"outputs":[],"source":["# Se importa la herramienta LabelEncoder de la librería sklearn.preprocessing\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Se crea el objeto le a partir de la clase Label Encoding\n","le = LabelEncoder()\n","\n","# Se hace una copia df2_norm del DataFrame df2_clean\n","df2_norm = df2_clean.copy()\n","\n","# Se crea una nueva columna en el DataFrame llamada \"target\"\n","# cuyos datos serán producto de aplicar el método fit_transform (va a asignar un número a cada especie de la columna 'Species)\n","df2_norm[\"target\"] = le.fit_transform(df2_norm['Species'])\n","\n","# Del DataFrame normalizado df_norm eliminar la anterior columna 'Species' que contiene los nombres de las especies.\n","df2_norm = df2_norm.drop([\"Species\"], axis=1)\n","\n","# Observar las 5 primeras filas del DataFrame resultante\n","df2_norm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jZJXT7g3q6D"},"outputs":[],"source":["# Se importa de sklearn.preprocessing la herramienta a usar  StandardScaler\n","from sklearn.preprocessing import StandardScaler\n","\n","# Se crea el objeto scaler2 a partir de la clase StandardScaler()\n","scaler2 = StandardScaler()\n","\n","# Del DataFrama df2_norm se implementa el operador .loc para editar las columnas 'Culmen Length (mm)','Culmen Depth (mm)','Culmen Depth (mm)','Flipper Length (mm)','Body Mass (g)'\n","# Para acceder a las columnas se utiliza los corchetes\n","# Los dos puntos (:) indican que se editarán todas las filas de la columna indicada\n","# Para normalizar se utilizá el objeto creado scaler2\n","# y el método .fit_transform, que se encargará de normalizar\n","# Entre paréntesis se indica el nombre de la columna del DataFrame. Ej:df2_norm[['Culmen Length (mm)']]\n","df2_norm.loc[:, 'Culmen Length (mm)'] = scaler2.fit_transform(df2_norm[['Culmen Length (mm)']])\n","df2_norm.loc[:, 'Culmen Depth (mm)'] = scaler2.fit_transform(df2_norm[['Culmen Depth (mm)']])\n","df2_norm.loc[:, 'Culmen Depth (mm)'] = scaler2.fit_transform(df2_norm[['Culmen Depth (mm)']])\n","df2_norm.loc[:, 'Flipper Length (mm)'] = scaler2.fit_transform(df2_norm[['Flipper Length (mm)']])\n","df2_norm.loc[:, 'Body Mass (g)'] = scaler2.fit_transform(df2_norm[['Body Mass (g)']])\n","\n","# Observar las 5 primeras filas\n","df2_norm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeMp7E4tnSwC"},"outputs":[],"source":["# Obtener los valores de X2 e y2\n","# En X2 se almacenarán todos los valores de las columnas (con .values), excepto los valores de la columna \"target\", la cuál se elimina con el método .drop()\n","# Necesita el nombre de la columna y\n","# axis=1 para que se elimine por filas.\n","X2 = df2_norm.drop(\"target\", axis=1).values\n","\n","# En y2, sólo se almacena los valores de la columna \"target\", que será la columna objetivo.\n","# Importante, se implementa to_categorical para obterner la información en un array de matrices, donde cada matriz contiene 4 valores, \n","# los cuatros valores de las categorias y que también representen las mismas cantidad de filas, es similar al onehotencoder.\n","# Necesita indicar la columna \"target\" del DataFrame df2_norm usando corchetes.\n","# se implementa el método values para obtener solo los valores y que no vengan incluidos los nombres de las columnas.\n","\n","y2 = to_categorical(df2_norm[\"target\"].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWT1oEuSiNAp"},"outputs":[],"source":["# Verificar las cantidad de filas y columnas en X2\n","X2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqNxYFdunzvA"},"outputs":[],"source":["# Verificar las cantidad de filas y columnas en y2\n","y2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZXKdXXmoFY-"},"outputs":[],"source":["# Se importa la herramienta de sklearn.model_selectionl como train_test_split\n","from sklearn.model_selection import train_test_split\n","\n","# Fijamos un \"random_state\" constante para que siempre el dataset se parta de la misma forma\n","# para poder repetir los ensayos\n","# Ojo! Los dataset de train y test son array numpy\n","# Se importa la herramienta de la libreria  train_test_split()\n","# Necesita los valores de X e y\n","# test_size=0.2, permite indicar el porcentaje de valores para validar, equivalente a un 20%\n","# random_state=42,  es un número fijo que utilizan comunmente en documentación, significa que para cada ejecución del algoritmo \n","#se genere nuevos valores aleatorios\n","# y los conjuntos de datos de entrenamiento y pruebas serán diferentes.\n","X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmudJksIfL8m"},"outputs":[],"source":["# Se importa Dense de la librería tensorflow.keras.layers\n","from tensorflow.keras.layers import Dense\n","\n","# Se crea el objeto model2 a partir de la clase Sequential()\n","model2 = Sequential()\n","\n","\n","# A partir del objeto creado se implementa el método add()\n","# Dentro de los () se indica que se ustilizará una capa Densa (Dense)\n","# Dentro de la capa densa se definen los parámetros:\n","# units, cantidad de neuronas\n","# activation='sigmoid', función de activación que define el corportamiento del modelo\n","# input_shape=(4,) cantidad de entradas, esto lo define la cantidad de columnas.\n","# Se repite el proceso con solo indicar las cantidad de neuronas y la función de activación al final\n","# que varia a 'softmax', ya que la predicción está representada por más de 2 categorías conocidas, es \n","# multicategórica.\n","\n","model2.add(Dense(units=3, activation='sigmoid', input_shape=(4,)))\n","model2.add(Dense(units=3, activation='softmax'))\n","\n","# De model2 creado se puede acceder al sumario que muestra la estructura del modelo\n","model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVR3AVkMfg_2"},"outputs":[],"source":["# Configuración del modelo para el entrenamiento, implementando el método compile a partir del modelo creado.\n","# Se necesita indicar los parámetros:\n","# optimizer, nombre del optimizador (es el algoritmo que se encarga del descenso de gradiente estocástico)\n","# Fuente: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n","# loss, se llama función de pérdida, representa las categorías conocidas de las predicción. Al ser 'binary_crossentropy' la predicción tiene \n","# una salida con dos opciones.\n","# metrics, se define la métrica que evaluará el modelo durante el entrenamiento y las pruebas.\n","model2.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n","# Se entrena el modelo con el método fit\n","# Necesita definir los valores para X, y sumado a la cantidad de épocas que seria la iteraciones de entrenamiento y el porcentaje\n","# dirigido a validación (validation_split=0.2)\n","history2 = model2.fit(X2_train, y2_train, validation_split=0.2, epochs=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNN1X4C7oeht"},"outputs":[],"source":["# Variable epocas_conteo, que almacena en una lista la cantidad de épocas de train\n","# history2, es la variable que almacena las predicciones del modelo\n","# y de ella se puede acceder a información como su historial (history) del accuracy\n","epocas_conteo = range(1, len(history2.history['accuracy']) + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDYWsWuo95Wc"},"outputs":[],"source":["# De Seaborn (sns) se accede al gráfico de línea para representar;\n","# Por un lado, el 'accuracy',\n","# Por el otro, la validación (val_accuracy)\n","sns.lineplot(x=epocas_conteo,  y=history2.history['accuracy'], label='train')\n","sns.lineplot(x=epocas_conteo,  y=history2.history['val_accuracy'], label='valid')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"vscode":{"interpreter":{"hash":"97e12f1ac9f26099b54b9e7d4ebc9834a88f2d401bac5ba802dc7f7f5939f7f3"}}},"nbformat":4,"nbformat_minor":0}
